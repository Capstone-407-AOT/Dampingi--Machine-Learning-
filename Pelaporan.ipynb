{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pelaporan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Moz0_nbtFl_k",
        "outputId": "0eef81a5-a595-4e20-bab4-6eb6a1481394"
      },
      "source": [
        "!pip install tflearn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tflearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/3c/0b156d08ef3d4e2a8009ecab2af1ad2e304f6fb99562b6271c68a74a4397/tflearn-0.5.0.tar.gz (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-cp37-none-any.whl size=127301 sha256=52741ab669e0fcb42c3913b2182c1f48c5b70e19dd95c96225359c1cba561697\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d2/ed/fb9a0d301dd9586c11e9547120278e624227f22fd5f4baf744\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW_GrBGGFFo3",
        "outputId": "3c0e22a6-2b46-4a12-8626-f3221baab9aa"
      },
      "source": [
        "#Imports\n",
        "import nltk\n",
        "import os\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "#Loading Data\n",
        "with open(\"training.json\") as file:\n",
        "\tdata = json.load(file)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl5-zaa0F635",
        "outputId": "749c32ab-2d12-4721-d0cb-6db23796416a"
      },
      "source": [
        "nltk.download('punkt')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCsitrxwFudt"
      },
      "source": [
        "#Initializing empty lists\n",
        "words = []\n",
        "labels = []\n",
        "docs_x = []\n",
        "docs_y = []\n",
        "\n",
        "#Looping through our data\n",
        "for intent in data['intents']:\n",
        "\tfor pattern in intent['patterns']:\n",
        "\t\tpattern = pattern.lower()\n",
        "    \t\t#Creating a list of words\n",
        "\t\twrds = nltk.word_tokenize(pattern)\n",
        "\t\twords.extend(wrds)\n",
        "\t\tdocs_x.append(wrds)\n",
        "\t\tdocs_y.append(intent['tag'])\n",
        "\n",
        "\tif intent['tag'] not in labels:\n",
        "\t  labels.append(intent['tag'])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SBuP2JCGClx",
        "outputId": "30716cf1-10b8-4965-91ff-7816e2bb3cfb"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['greeting', 'Konfirmasi_nama', 'Konfirmasi_NIK', 'Konfirmasi_alamat', 'diagnosa', 'Konfirmasi_diagnosa', 'K_anak_apa', 'k_anak_dimana', 'k_anak_Kapan', 'k_anak_siapa', 'k_anak_siapa_yang_melapor']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0MdnT6BGWx3"
      },
      "source": [
        "stemmer = LancasterStemmer()\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
        "words = sorted(list(set(words)))\n",
        "labels = sorted(labels)\n",
        "\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "for x,doc in enumerate(docs_x):\n",
        "\tbag = []\n",
        "\twrds = [stemmer.stem(w) for w in doc]\n",
        "\tfor w in words:\n",
        "\t\tif w in wrds:\n",
        "\t\t\tbag.append(1)\n",
        "\t\telse:\n",
        "\t\t\tbag.append(0)\n",
        "\toutput_row = out_empty[:]\n",
        "\toutput_row[labels.index(docs_y[x])] = 1\n",
        "\ttraining.append(bag)\n",
        "\toutput.append(output_row)\n",
        "#Converting training data into NumPy arrays\n",
        "training = np.array(training)\n",
        "output = np.array(output)\n",
        "\n",
        "#Saving data to disk\n",
        "with open(\"data.pickle\",\"wb\") as f:\n",
        "\tpickle.dump((words, labels, training, output),f)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elKhVoWoG6W4"
      },
      "source": [
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3AT52mBGofF",
        "outputId": "e3c7df28-e878-4c64-d48e-3e19504d041d"
      },
      "source": [
        "ops.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape = [None, len(training[0])])\n",
        "net = tflearn.fully_connected(net,8)\n",
        "net = tflearn.fully_connected(net,8)\n",
        "net = tflearn.fully_connected(net,len(output[0]), activation = \"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLmtWSabHDt_",
        "outputId": "d4c0b051-5695-4b69-f2fe-ac8afac9edd7"
      },
      "source": [
        "model.fit(training, output, n_epoch = 200, batch_size = 8, show_metric = True)\n",
        "model.save(\"model.tflearn\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 1799  | total loss: \u001b[1m\u001b[32m1.56465\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 200 | loss: 1.56465 - acc: 0.5597 -- iter: 64/66\n",
            "Training Step: 1800  | total loss: \u001b[1m\u001b[32m1.48619\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 200 | loss: 1.48619 - acc: 0.5483 -- iter: 66/66\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N07yw_PHxFw"
      },
      "source": [
        "## Coba inferensi\n",
        "\n",
        "def bag_of_words(s, words):\n",
        "\tbag = [0 for _ in range(len(words))]\n",
        "\t\n",
        "\ts_words = nltk.word_tokenize(s)\n",
        "\ts_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "\tfor se in s_words:\n",
        "\t\tfor i,w in enumerate(words):\n",
        "\t\t\tif w == se:\n",
        "\t\t\t\tbag[i] = 1\n",
        "\n",
        "\treturn np.array(bag)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnzEOUtyIJBB",
        "outputId": "0dd94c6d-2543-414f-b3d2-0533ac4323c4"
      },
      "source": [
        "message = \"orangtua\"\n",
        "results = model.predict([bag_of_words(message,words)])[0]\n",
        "print(results)\n",
        "\n",
        "result_index = np.argmax(results)\n",
        "print(result_index)\n",
        "tag = labels[result_index]\n",
        "print(tag)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.2367582e-02 8.3419718e-03 5.9065386e-03 7.3194075e-03 6.9327280e-03\n",
            " 2.4391701e-02 5.0235790e-06 8.1296303e-06 1.6743914e-04 6.3481623e-01\n",
            " 2.8974324e-01]\n",
            "9\n",
            "k_anak_siapa\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}